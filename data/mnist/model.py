# Autogenerated by onnx-pytorch.
# %%
import glob
import os
import math

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision


class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self._vars = nn.ParameterDict()
        self._regularizer_params = []
        d = os.path.join(os.path.dirname(__file__), "variables", "*.npy")
        print(f"loading weights from {d}")

        for b in glob.glob(d):
            # print(b)
            v = torch.from_numpy(np.load(b))
            requires_grad = v.dtype.is_floating_point or v.dtype.is_complex
            self._vars[os.path.basename(b)[:-4]] = nn.Parameter(
                v, requires_grad=requires_grad
            )
        self.n_Convolution28 = nn.Conv2d(
            **{
                "groups": 1,
                "dilation": [1, 1],
                "out_channels": 8,
                "padding": 0,
                "kernel_size": (5, 5),
                "stride": [1, 1],
                "in_channels": 1,
                "bias": False,
            }
        )
        self.n_Convolution28.weight.data = self._vars["Parameter5"]
        self.n_Pooling66 = nn.MaxPool2d(
            **{
                "dilation": 1,
                "kernel_size": [2, 2],
                "ceil_mode": False,
                "stride": [2, 2],
                "return_indices": False,
                "padding": [0, 0],
            }
        )
        self.n_Convolution110 = nn.Conv2d(
            **{
                "groups": 1,
                "dilation": [1, 1],
                "out_channels": 16,
                "padding": 0,
                "kernel_size": (5, 5),
                "stride": [1, 1],
                "in_channels": 8,
                "bias": False,
            }
        )
        self.n_Convolution110.weight.data = self._vars["Parameter87"]
        self.n_Pooling160 = nn.MaxPool2d(
            **{
                "dilation": 1,
                "kernel_size": [3, 3],
                "ceil_mode": False,
                "stride": [3, 3],
                "return_indices": False,
                "padding": [0, 0],
            }
        )

    def forward(self, *inputs):
        (Input3,) = inputs
        self._vars["Pooling160_Output_0_reshape0_shape"][0] = Input3.shape[0] # batch size 设置
        
        Parameter193_reshape1 = torch.reshape(
            self._vars["Parameter193"],
            [
                s if s != 0 else self._vars["Parameter193"].shape[i]
                for i, s in enumerate(self._vars["Parameter193_reshape1_shape"])
            ],
        )
        Input3 = self.compatible_auto_pad(
            Input3,
            self.n_Convolution28.weight.data.shape[2:],
            self.n_Convolution28,
            "SAME_UPPER",
        )
        Convolution28_Output_0 = self.n_Convolution28(Input3)
        Plus30_Output_0 = torch.add(Convolution28_Output_0, self._vars["Parameter6"])
        ReLU32_Output_0 = F.relu(Plus30_Output_0)
        Pooling66_Output_0 = self.n_Pooling66(ReLU32_Output_0)
        Pooling66_Output_0 = self.compatible_auto_pad(
            Pooling66_Output_0,
            self.n_Convolution110.weight.data.shape[2:],
            self.n_Convolution110,
            "SAME_UPPER",
        )
        Convolution110_Output_0 = self.n_Convolution110(Pooling66_Output_0)
        Plus112_Output_0 = torch.add(Convolution110_Output_0, self._vars["Parameter88"])
        ReLU114_Output_0 = F.relu(Plus112_Output_0)
        # 上面都正常
        Pooling160_Output_0 = self.n_Pooling160(ReLU114_Output_0)
        Pooling160_Output_0_reshape0 = torch.reshape(
            Pooling160_Output_0,
            [
                s if s != 0 else Pooling160_Output_0.shape[i]
                for i, s in enumerate(self._vars["Pooling160_Output_0_reshape0_shape"])
            ],
        )
        Times212_Output_0 = torch.matmul(
            Pooling160_Output_0_reshape0, Parameter193_reshape1
        )
        Plus214_Output_0 = torch.add(Times212_Output_0, self._vars["Parameter194"])
        return Plus214_Output_0

    def compatible_auto_pad(
        self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs
    ):
        input_spatial_shape = input.shape[2:]
        d = len(input_spatial_shape)
        strides = nn_mod.stride
        dilations = nn_mod.dilation
        output_spatial_shape = [
            math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)
        ]
        pt_padding = [0] * 2 * d
        pad_shape = [0] * d
        for i in range(d):
            pad_shape[i] = (
                (output_spatial_shape[i] - 1) * strides[i]
                + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1)
                - input_spatial_shape[i]
            )
            mean = pad_shape[i] // 2
            if auto_pad == b"SAME_UPPER":
                l, r = pad_shape[i] - mean, mean
            else:
                l, r = mean, pad_shape[i] - mean
            pt_padding.insert(0, r)
            pt_padding.insert(0, l)
        return F.pad(input, pt_padding)


# @torch.no_grad()
# def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 1, 28, 28]).astype(np.float32))]):
def test_run_model(
    inputs=[torch.from_numpy(np.random.randn(*[2, 1, 28, 28]).astype(np.float32))]
):
    model = Model()
    model.eval()
    rs = model(*inputs)
    print(rs.shape)
    return rs


# %%
test_run_model()
#%%
# 以图片方式显示 learning_x[0][0]
def show(tensor):
  from PIL import Image
  import matplotlib.pyplot as plt
  plt.imshow(tensor.detach().numpy())
# inputs=[torch.from_numpy(np.random.randn(*[2048, 1, 28, 28]).astype(np.float32))]
# learning_x = torch.randn((2048, 1, 28, 28), dtype=torch.float32, requires_grad=True)
# learning_x = torch.zeros((2048, 1, 28, 28), dtype=torch.float32, requires_grad=True)
learning_x = torch.ones((2048, 1, 28, 28), 
                        dtype=torch.float32)*255
learning_x.requires_grad_(True)
show(learning_x[0][0])
#%%
from mnist import MNIST
mndata = MNIST("..")  # Replace with the path to your MNIST data folder
images, labels = mndata.load_testing()
learning_x = torch.from_numpy(np.array(images[:2048]).reshape(2048, 1, 28, 28).astype(np.float32))
learning_x.requires_grad_(True)
learning_x.shape
# %%
model = Model()
model.train()
model.requires_grad_(False)

rs = model(*[learning_x])
criterion = nn.CrossEntropyLoss()
label = torch.zeros((2048, 10), dtype=torch.float32)
# label[:, 0] = 1
label[:, 4] = 1
# optimizer = torch.optim.Adam([learning_x], lr=1e-1)
optimizer = torch.optim.Adam([learning_x], lr=1)
# optimizer = torch.optim.AdamW([learning_x], lr=10, 
#                               weight_decay=0.1
#                               )
from tqdm import tqdm
bar = tqdm(range(100))
for i in bar:
    optimizer.zero_grad()
    rs = model(*[learning_x])
    loss = criterion(rs, label)
    loss.backward()
    optimizer.step()
    bar.set_postfix(loss=loss.item())
# %%
torch.argmax(model(*[learning_x]), dim=1)
labels[:2048]
# %%
# show(learning_x[1][0])
for i in range(5):
  plt.figure()
  show(learning_x[i][0])
# %%
